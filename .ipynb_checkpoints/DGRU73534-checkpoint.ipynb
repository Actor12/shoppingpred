{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.6/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，简单处理list数据\n",
    "train = pd.read_csv('./data/train.txt', header=None)\n",
    "test = pd.read_csv('./data/test.txt', header=None)\n",
    "\n",
    "train.columns = ['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "test.columns = ['pid', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "\n",
    "train['label'] = train['label'].astype(int)\n",
    "\n",
    "data = pd.concat([train,test])\n",
    "data['label'] = data['label'].fillna(-1)\n",
    "\n",
    "data['tagid'] = data['tagid'].apply(lambda x:eval(x))\n",
    "data['tagid'] = data['tagid'].apply(lambda x:[str(i) for i in x])\n",
    "\n",
    "# 超参数\n",
    "# embed_size  embedding size\n",
    "# MAX_NB_WORDS  tagid中的单词出现次数\n",
    "# MAX_SEQUENCE_LENGTH  输入tagid list的长度\n",
    "embed_size = 256   #64\n",
    "MAX_NB_WORDS = 230637\n",
    "MAX_SEQUENCE_LENGTH = 300   #128\n",
    "# 训练word2vec，这里可以考虑elmo，bert等预训练\n",
    "w2v_model = Word2Vec(sentences=data['tagid'].tolist(), vector_size=embed_size, window=5, min_count=1,epochs=7)\n",
    "#w2v_model.save(\"./w2vmodel/w2vmodel.model\")\n",
    "#w2v_model = Word2Vec.load(\"./w2vmodel/w2vmodel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 230638 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 这里是划分训练集和测试数据\n",
    "X_train = data[:train.shape[0]]['tagid']\n",
    "X_test = data[train.shape[0]:]['tagid']\n",
    "\n",
    "# 创建词典，利用了tf.keras的API，其实就是编码一下，具体可以看看API的使用方法\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "# 计算一共出现了多少个单词，其实MAX_NB_WORDS我直接就用了这个数据\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "print('Total %s word vectors.' % nb_words)\n",
    "# 构建一个embedding的矩阵，之后输入到模型使用\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "        #print(np.shape(embedding_vector))\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "y_categorical = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230638, 300000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols= [\"tagidEmb_\"+str(i+1) for i in range(128)]\n",
    "dfem = pd.DataFrame(data=embedding_matrix,columns=cols)\n",
    "len(dfem),len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n1\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 256)          59043328  \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 300, 512)          1182720   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300, 512)          2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300, 512)          0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 512)               1575936   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 61,937,665\n",
      "Trainable params: 2,892,289\n",
      "Non-trainable params: 59,045,376\n",
      "_________________________________________________________________\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 214s 793us/sample - loss: 0.6057 - accuracy: 0.6705 - val_loss: 0.6236 - val_accuracy: 0.6551\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.5629 - accuracy: 0.7035 - val_loss: 0.5623 - val_accuracy: 0.7081\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.5411 - accuracy: 0.7205 - val_loss: 0.6318 - val_accuracy: 0.6431\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 210s 779us/sample - loss: 0.5215 - accuracy: 0.7344 - val_loss: 0.5321 - val_accuracy: 0.7237\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5042 - accuracy: 0.7459 - val_loss: 0.5171 - val_accuracy: 0.7379\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.4876 - accuracy: 0.7564 - val_loss: 0.5366 - val_accuracy: 0.7231\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4720 - accuracy: 0.7652 - val_loss: 0.5043 - val_accuracy: 0.7441\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.4565 - accuracy: 0.7739 - val_loss: 0.5110 - val_accuracy: 0.7387\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4393 - accuracy: 0.7849 - val_loss: 0.4992 - val_accuracy: 0.7477\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4202 - accuracy: 0.7956 - val_loss: 0.5060 - val_accuracy: 0.7446\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3991 - accuracy: 0.8074 - val_loss: 0.5215 - val_accuracy: 0.7412\n",
      "Epoch 12/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3769 - accuracy: 0.8204\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 209s 772us/sample - loss: 0.3768 - accuracy: 0.8204 - val_loss: 0.5899 - val_accuracy: 0.7395\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3122 - accuracy: 0.8549 - val_loss: 0.6227 - val_accuracy: 0.7404\n",
      "Epoch 14/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.2848 - accuracy: 0.8673 - val_loss: 0.6714 - val_accuracy: 0.7375\n",
      "Epoch 00014: early stopping\n",
      "[[0.02272191]\n",
      " [0.04801544]\n",
      " [0.05480007]\n",
      " ...\n",
      " [0.04579886]\n",
      " [0.04185071]\n",
      " [0.02453247]]\n",
      "fold n2\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 214s 792us/sample - loss: 0.6115 - accuracy: 0.6673 - val_loss: 0.5795 - val_accuracy: 0.6899\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.5653 - accuracy: 0.7015 - val_loss: 0.6223 - val_accuracy: 0.6539\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5375 - accuracy: 0.7226 - val_loss: 0.5386 - val_accuracy: 0.7226\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5149 - accuracy: 0.7379 - val_loss: 0.5303 - val_accuracy: 0.7316\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4934 - accuracy: 0.7523 - val_loss: 0.5186 - val_accuracy: 0.7342\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4717 - accuracy: 0.7662 - val_loss: 0.5174 - val_accuracy: 0.7415\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 209s 772us/sample - loss: 0.4490 - accuracy: 0.7801 - val_loss: 0.5456 - val_accuracy: 0.7305\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4226 - accuracy: 0.7944 - val_loss: 0.5268 - val_accuracy: 0.7447\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.3916 - accuracy: 0.8124 - val_loss: 0.6250 - val_accuracy: 0.7208\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.3588 - accuracy: 0.8297 - val_loss: 0.5709 - val_accuracy: 0.7390\n",
      "Epoch 11/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3249 - accuracy: 0.8480\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.3250 - accuracy: 0.8479 - val_loss: 0.6340 - val_accuracy: 0.7309\n",
      "Epoch 12/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.2395 - accuracy: 0.8918 - val_loss: 0.7944 - val_accuracy: 0.7293\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.2022 - accuracy: 0.9085 - val_loss: 0.9176 - val_accuracy: 0.7271\n",
      "Epoch 00013: early stopping\n",
      "[[0.03033802]\n",
      " [0.09375434]\n",
      " [0.09795757]\n",
      " ...\n",
      " [0.08196454]\n",
      " [0.09974976]\n",
      " [0.03157229]]\n",
      "fold n3\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 215s 796us/sample - loss: 0.6064 - accuracy: 0.6716 - val_loss: 0.5796 - val_accuracy: 0.6907\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5605 - accuracy: 0.7050 - val_loss: 0.5687 - val_accuracy: 0.6989\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 211s 782us/sample - loss: 0.5374 - accuracy: 0.7229 - val_loss: 0.5509 - val_accuracy: 0.7078\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.5188 - accuracy: 0.7358 - val_loss: 0.5846 - val_accuracy: 0.7009\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 211s 780us/sample - loss: 0.5029 - accuracy: 0.7455 - val_loss: 0.5290 - val_accuracy: 0.7257\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.4885 - accuracy: 0.7548 - val_loss: 0.5442 - val_accuracy: 0.7278\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 211s 781us/sample - loss: 0.4774 - accuracy: 0.7615 - val_loss: 0.5129 - val_accuracy: 0.7406\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 211s 782us/sample - loss: 0.4668 - accuracy: 0.7678 - val_loss: 0.4954 - val_accuracy: 0.7468\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4549 - accuracy: 0.7751 - val_loss: 0.7559 - val_accuracy: 0.6988\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4430 - accuracy: 0.7819 - val_loss: 0.5089 - val_accuracy: 0.7475\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.4321 - accuracy: 0.7881 - val_loss: 0.5044 - val_accuracy: 0.7509\n",
      "Epoch 12/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.4212 - accuracy: 0.7943 - val_loss: 0.5170 - val_accuracy: 0.7388\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 209s 776us/sample - loss: 0.4073 - accuracy: 0.8026 - val_loss: 0.5048 - val_accuracy: 0.7442\n",
      "Epoch 14/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8097\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.3942 - accuracy: 0.8098 - val_loss: 0.6006 - val_accuracy: 0.7327\n",
      "Epoch 15/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.3453 - accuracy: 0.8367 - val_loss: 0.5590 - val_accuracy: 0.7458\n",
      "Epoch 16/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.3251 - accuracy: 0.8463 - val_loss: 0.6004 - val_accuracy: 0.7465\n",
      "Epoch 00016: early stopping\n",
      "[[0.04066158]\n",
      " [0.14738832]\n",
      " [0.14470489]\n",
      " ...\n",
      " [0.12671003]\n",
      " [0.15473107]\n",
      " [0.0685311 ]]\n",
      "fold n4\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 213s 789us/sample - loss: 0.6079 - accuracy: 0.6686 - val_loss: 0.6955 - val_accuracy: 0.6317\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5638 - accuracy: 0.7024 - val_loss: 0.6328 - val_accuracy: 0.6443\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5373 - accuracy: 0.7226 - val_loss: 0.5278 - val_accuracy: 0.7263\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5143 - accuracy: 0.7377 - val_loss: 0.5107 - val_accuracy: 0.7404\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4955 - accuracy: 0.7505 - val_loss: 0.5356 - val_accuracy: 0.7177\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4766 - accuracy: 0.7630 - val_loss: 0.5447 - val_accuracy: 0.7109\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.4572 - accuracy: 0.7740 - val_loss: 0.5126 - val_accuracy: 0.7422\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.4344 - accuracy: 0.7880 - val_loss: 0.5407 - val_accuracy: 0.7421\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4079 - accuracy: 0.8035 - val_loss: 0.5242 - val_accuracy: 0.7427\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3799 - accuracy: 0.8177 - val_loss: 0.5228 - val_accuracy: 0.7427\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.3489 - accuracy: 0.8355 - val_loss: 0.5937 - val_accuracy: 0.7392\n",
      "Epoch 12/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8505\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.3193 - accuracy: 0.8505 - val_loss: 0.5846 - val_accuracy: 0.7383\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.2391 - accuracy: 0.8912 - val_loss: 0.8026 - val_accuracy: 0.7331\n",
      "Epoch 14/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.2045 - accuracy: 0.9070 - val_loss: 0.9238 - val_accuracy: 0.7317\n",
      "Epoch 15/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9157\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.000000136438758e-06.\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.1846 - accuracy: 0.9157 - val_loss: 1.0082 - val_accuracy: 0.7304\n",
      "Epoch 00015: early stopping\n",
      "[[0.07079922]\n",
      " [0.19427318]\n",
      " [0.1854598 ]\n",
      " ...\n",
      " [0.17419017]\n",
      " [0.21616612]\n",
      " [0.09328542]]\n",
      "fold n5\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 213s 790us/sample - loss: 0.6095 - accuracy: 0.6671 - val_loss: 0.5842 - val_accuracy: 0.6898\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5684 - accuracy: 0.6998 - val_loss: 0.5637 - val_accuracy: 0.7010\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5450 - accuracy: 0.7179 - val_loss: 0.5456 - val_accuracy: 0.7143\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5195 - accuracy: 0.7351 - val_loss: 0.5240 - val_accuracy: 0.7330\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.4997 - accuracy: 0.7481 - val_loss: 0.5253 - val_accuracy: 0.7336\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4772 - accuracy: 0.7624 - val_loss: 0.5072 - val_accuracy: 0.7439\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.4505 - accuracy: 0.7785 - val_loss: 0.5255 - val_accuracy: 0.7367\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 209s 772us/sample - loss: 0.4190 - accuracy: 0.7969 - val_loss: 0.5248 - val_accuracy: 0.7375\n",
      "Epoch 9/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3829 - accuracy: 0.8176\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.3829 - accuracy: 0.8176 - val_loss: 0.5683 - val_accuracy: 0.7389\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.2907 - accuracy: 0.8648 - val_loss: 0.6850 - val_accuracy: 0.7338\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.2525 - accuracy: 0.8835 - val_loss: 0.7672 - val_accuracy: 0.7301\n",
      "Epoch 00011: early stopping\n",
      "[[0.07982741]\n",
      " [0.22822977]\n",
      " [0.22369662]\n",
      " ...\n",
      " [0.2169621 ]\n",
      " [0.27661758]\n",
      " [0.10570043]]\n",
      "fold n6\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 214s 794us/sample - loss: 0.6099 - accuracy: 0.6673 - val_loss: 0.5809 - val_accuracy: 0.6891\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.5647 - accuracy: 0.7016 - val_loss: 0.6085 - val_accuracy: 0.6742\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5360 - accuracy: 0.7238 - val_loss: 0.5337 - val_accuracy: 0.7264\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.5164 - accuracy: 0.7374 - val_loss: 0.6007 - val_accuracy: 0.6723\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 210s 776us/sample - loss: 0.5018 - accuracy: 0.7467 - val_loss: 0.5111 - val_accuracy: 0.7438\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4845 - accuracy: 0.7578 - val_loss: 0.5101 - val_accuracy: 0.7448\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4678 - accuracy: 0.7680 - val_loss: 0.5088 - val_accuracy: 0.7478\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4504 - accuracy: 0.7782 - val_loss: 0.5215 - val_accuracy: 0.7439\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4297 - accuracy: 0.7896 - val_loss: 0.5076 - val_accuracy: 0.7471\n",
      "Epoch 10/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.4072 - accuracy: 0.8030\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4073 - accuracy: 0.8030 - val_loss: 0.5140 - val_accuracy: 0.7441\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3432 - accuracy: 0.8383 - val_loss: 0.5837 - val_accuracy: 0.7447\n",
      "Epoch 12/128\n",
      "270000/270000 [==============================] - 209s 772us/sample - loss: 0.3169 - accuracy: 0.8519 - val_loss: 0.5997 - val_accuracy: 0.7448\n",
      "Epoch 00012: early stopping\n",
      "[[0.09078489]\n",
      " [0.28211095]\n",
      " [0.24626216]\n",
      " ...\n",
      " [0.27834372]\n",
      " [0.32990269]\n",
      " [0.12977053]]\n",
      "fold n7\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 214s 792us/sample - loss: 0.6054 - accuracy: 0.6708 - val_loss: 0.5784 - val_accuracy: 0.6887\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.5610 - accuracy: 0.7042 - val_loss: 0.5795 - val_accuracy: 0.6897\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 210s 779us/sample - loss: 0.5375 - accuracy: 0.7232 - val_loss: 0.5330 - val_accuracy: 0.7267\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.5190 - accuracy: 0.7353 - val_loss: 0.5742 - val_accuracy: 0.7052\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 211s 781us/sample - loss: 0.5048 - accuracy: 0.7454 - val_loss: 0.5296 - val_accuracy: 0.7283\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.4929 - accuracy: 0.7517 - val_loss: 0.5291 - val_accuracy: 0.7276\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 207s 767us/sample - loss: 0.4808 - accuracy: 0.7597 - val_loss: 0.5311 - val_accuracy: 0.7388\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4717 - accuracy: 0.7649 - val_loss: 0.5289 - val_accuracy: 0.7468\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.4612 - accuracy: 0.7720 - val_loss: 0.5160 - val_accuracy: 0.7496\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 207s 768us/sample - loss: 0.4518 - accuracy: 0.7780 - val_loss: 0.5734 - val_accuracy: 0.7426\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4429 - accuracy: 0.7818 - val_loss: 0.5032 - val_accuracy: 0.7446\n",
      "Epoch 12/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.4336 - accuracy: 0.7879 - val_loss: 0.4992 - val_accuracy: 0.7504\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 208s 769us/sample - loss: 0.4243 - accuracy: 0.7937 - val_loss: 0.4995 - val_accuracy: 0.7455\n",
      "Epoch 14/128\n",
      "270000/270000 [==============================] - 208s 769us/sample - loss: 0.4133 - accuracy: 0.7986 - val_loss: 0.5121 - val_accuracy: 0.7423\n",
      "Epoch 15/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8054\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4025 - accuracy: 0.8054 - val_loss: 0.5249 - val_accuracy: 0.7436\n",
      "Epoch 16/128\n",
      "270000/270000 [==============================] - 207s 768us/sample - loss: 0.3615 - accuracy: 0.8275 - val_loss: 0.5328 - val_accuracy: 0.7500\n",
      "Epoch 17/128\n",
      "270000/270000 [==============================] - 207s 768us/sample - loss: 0.3450 - accuracy: 0.8365 - val_loss: 0.5486 - val_accuracy: 0.7488\n",
      "Epoch 00017: early stopping\n",
      "[[0.1298128 ]\n",
      " [0.31514368]\n",
      " [0.2817025 ]\n",
      " ...\n",
      " [0.3121694 ]\n",
      " [0.36814349]\n",
      " [0.14669284]]\n",
      "fold n8\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 214s 794us/sample - loss: 0.6110 - accuracy: 0.6678 - val_loss: 0.5739 - val_accuracy: 0.6920\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 208s 770us/sample - loss: 0.5668 - accuracy: 0.7004 - val_loss: 0.6648 - val_accuracy: 0.6446\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 209s 776us/sample - loss: 0.5390 - accuracy: 0.7214 - val_loss: 0.5296 - val_accuracy: 0.7251\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 208s 772us/sample - loss: 0.5165 - accuracy: 0.7367 - val_loss: 0.5340 - val_accuracy: 0.7191\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 210s 777us/sample - loss: 0.4993 - accuracy: 0.7478 - val_loss: 0.5075 - val_accuracy: 0.7401\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4816 - accuracy: 0.7600 - val_loss: 0.5362 - val_accuracy: 0.7389\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4622 - accuracy: 0.7713 - val_loss: 0.5298 - val_accuracy: 0.7349\n",
      "Epoch 8/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.4401 - accuracy: 0.7835\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 208s 770us/sample - loss: 0.4402 - accuracy: 0.7835 - val_loss: 0.5228 - val_accuracy: 0.7283\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 207s 769us/sample - loss: 0.3785 - accuracy: 0.8194 - val_loss: 0.5319 - val_accuracy: 0.7449\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 208s 770us/sample - loss: 0.3516 - accuracy: 0.8334 - val_loss: 0.5490 - val_accuracy: 0.7438\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.3332 - accuracy: 0.8431 - val_loss: 0.5711 - val_accuracy: 0.7417\n",
      "Epoch 12/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.8507\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.000000136438758e-06.\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.3172 - accuracy: 0.8507 - val_loss: 0.5891 - val_accuracy: 0.7390\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.2978 - accuracy: 0.8611 - val_loss: 0.6239 - val_accuracy: 0.7400\n",
      "Epoch 14/128\n",
      "270000/270000 [==============================] - 208s 769us/sample - loss: 0.2954 - accuracy: 0.8619 - val_loss: 0.6318 - val_accuracy: 0.7396\n",
      "Epoch 00014: early stopping\n",
      "[[0.16245043]\n",
      " [0.35004371]\n",
      " [0.32282987]\n",
      " ...\n",
      " [0.36254794]\n",
      " [0.42047233]\n",
      " [0.17373193]]\n",
      "fold n9\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 213s 788us/sample - loss: 0.6101 - accuracy: 0.6694 - val_loss: 0.5792 - val_accuracy: 0.6894\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.5683 - accuracy: 0.6994 - val_loss: 0.5585 - val_accuracy: 0.7086\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5430 - accuracy: 0.7192 - val_loss: 0.5262 - val_accuracy: 0.7277\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.5177 - accuracy: 0.7366 - val_loss: 0.5489 - val_accuracy: 0.7213\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 210s 779us/sample - loss: 0.4993 - accuracy: 0.7487 - val_loss: 0.5156 - val_accuracy: 0.7350\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4804 - accuracy: 0.7611 - val_loss: 0.5070 - val_accuracy: 0.7403\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4599 - accuracy: 0.7725 - val_loss: 0.5081 - val_accuracy: 0.7437\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4360 - accuracy: 0.7865 - val_loss: 0.5240 - val_accuracy: 0.7425\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.4086 - accuracy: 0.8032 - val_loss: 0.5201 - val_accuracy: 0.7429\n",
      "Epoch 10/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8198\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3793 - accuracy: 0.8198 - val_loss: 0.5700 - val_accuracy: 0.7297\n",
      "Epoch 11/128\n",
      "270000/270000 [==============================] - 209s 774us/sample - loss: 0.2997 - accuracy: 0.8613 - val_loss: 0.6332 - val_accuracy: 0.7381\n",
      "Epoch 12/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.2655 - accuracy: 0.8780 - val_loss: 0.6974 - val_accuracy: 0.7349\n",
      "Epoch 00012: early stopping\n",
      "[[0.18648426]\n",
      " [0.39721007]\n",
      " [0.36275259]\n",
      " ...\n",
      " [0.41597485]\n",
      " [0.49027379]\n",
      " [0.18933902]]\n",
      "fold n10\n",
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/128\n",
      "270000/270000 [==============================] - 214s 791us/sample - loss: 0.6102 - accuracy: 0.6680 - val_loss: 0.5763 - val_accuracy: 0.6923\n",
      "Epoch 2/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.5687 - accuracy: 0.7000 - val_loss: 0.5634 - val_accuracy: 0.7041\n",
      "Epoch 3/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.5449 - accuracy: 0.7176 - val_loss: 0.5540 - val_accuracy: 0.7114\n",
      "Epoch 4/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.5222 - accuracy: 0.7328 - val_loss: 0.5321 - val_accuracy: 0.7246\n",
      "Epoch 5/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.5036 - accuracy: 0.7455 - val_loss: 0.5200 - val_accuracy: 0.7356\n",
      "Epoch 6/128\n",
      "270000/270000 [==============================] - 209s 775us/sample - loss: 0.4837 - accuracy: 0.7583 - val_loss: 0.5104 - val_accuracy: 0.7396\n",
      "Epoch 7/128\n",
      "270000/270000 [==============================] - 210s 778us/sample - loss: 0.4624 - accuracy: 0.7714 - val_loss: 0.5044 - val_accuracy: 0.7410\n",
      "Epoch 8/128\n",
      "270000/270000 [==============================] - 208s 771us/sample - loss: 0.4379 - accuracy: 0.7855 - val_loss: 0.5232 - val_accuracy: 0.7428\n",
      "Epoch 9/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.4101 - accuracy: 0.8012 - val_loss: 0.5552 - val_accuracy: 0.7392\n",
      "Epoch 10/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3789 - accuracy: 0.8182 - val_loss: 0.5953 - val_accuracy: 0.7376\n",
      "Epoch 11/128\n",
      "269700/270000 [============================>.] - ETA: 0s - loss: 0.3475 - accuracy: 0.8350\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.999999845400454e-05.\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.3475 - accuracy: 0.8350 - val_loss: 0.6857 - val_accuracy: 0.7171\n",
      "Epoch 12/128\n",
      "270000/270000 [==============================] - 209s 773us/sample - loss: 0.2629 - accuracy: 0.8788 - val_loss: 0.7438 - val_accuracy: 0.7330\n",
      "Epoch 13/128\n",
      "270000/270000 [==============================] - 207s 768us/sample - loss: 0.2276 - accuracy: 0.8951 - val_loss: 0.8340 - val_accuracy: 0.7302\n",
      "Epoch 00013: early stopping\n",
      "[[0.20757286]\n",
      " [0.42387678]\n",
      " [0.40903316]\n",
      " ...\n",
      " [0.45829979]\n",
      " [0.54838436]\n",
      " [0.19806125]]\n"
     ]
    }
   ],
   "source": [
    "def my_model():\n",
    "    embedding_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(nb_words,\n",
    "                         embed_size,\n",
    "                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                          weights=[embedding_matrix],\n",
    "                         trainable=False      #考虑解冻？过拟合了\n",
    "                         )\n",
    "    embed = embedder(embedding_input)\n",
    "    g1 = GRU(units=512, return_sequences=True)(embed)\n",
    "    bn1 = BatchNormalization()(g1)\n",
    "    drop1 = Dropout(0.15)(bn1)\n",
    "    g2 = GRU(units=512)(drop1)\n",
    "    bn2 = BatchNormalization()(g2)\n",
    "    drop2 = Dropout(0.15)(bn2)\n",
    "    d2 = Dense(256,activation='relu')(drop2)\n",
    "    #gap1 =AveragePooling1D()(drop2)\n",
    "    main_output = Dense(1, activation='sigmoid')(d2)\n",
    "    model = Model(inputs=embedding_input, outputs=main_output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=Adam(lr=0.0009), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 五折交叉验证\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2048)\n",
    "oof = np.zeros([len(train), 1])\n",
    "predictions = np.zeros([len(test), 1])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "    print(\"fold n{}\".format(fold_ + 1))\n",
    "    model = my_model()\n",
    "    #model.load_weights('./model/DGRU74341_73508/DGRU_5.h5')\n",
    "    if fold_ == 0:\n",
    "        model.summary()\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, mode='auto',min_delta=0.0001, verbose=2)  #改监控指标为val_acc\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5,verbose=2)\n",
    "    bst_model_path = \"./model/DGRU_{}.h5\".format(fold_+1)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    X_tra, X_val = X_train[trn_idx], X_train[val_idx]\n",
    "    y_tra, y_val = y_categorical[trn_idx], y_categorical[val_idx]\n",
    "\n",
    "    model.fit(X_tra, y_tra,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=128, batch_size=300, shuffle=True,\n",
    "              callbacks=[early_stopping, model_checkpoint,reduce_lr])\n",
    "\n",
    "    model.load_weights(bst_model_path)\n",
    "\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "\n",
    "    predictions += model.predict(X_test) / folds.n_splits\n",
    "    print(predictions)\n",
    "    del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:0.7432266666666667\n"
     ]
    }
   ],
   "source": [
    "tmp = train[['pid','label']].copy()\n",
    "tmp['predict'] = oof\n",
    "tmp['rank'] = tmp['predict'].rank()\n",
    "tmp['p'] = 1\n",
    "tmp.loc[tmp['rank'] <= tmp.shape[0] * 0.5, 'p'] = 0\n",
    "bst_f1_tmp = f1_score(tmp['label'].values, tmp['p'].values)\n",
    "print('f1_score:{}'.format(bst_f1_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:0.7432266666666667\n"
     ]
    }
   ],
   "source": [
    "tmp1 = train[['pid','label']].copy()\n",
    "tmp1['predict'] = oof\n",
    "tmp1 = tmp1.sort_values(['predict'],ascending=False)\n",
    "tmp1 = tmp1.reset_index(drop=True)\n",
    "tmp1['p'] = 1\n",
    "tmp1.loc[len(tmp1)*0.5:, 'p'] = 0\n",
    "bst_f1_tmp = f1_score(tmp1['label'].values, tmp1['p'].values)\n",
    "print('f1_score:{}'.format(bst_f1_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = test[['pid']]\n",
    "submit['proba'] = predictions\n",
    "submit.columns = ['user_id', 'proba']\n",
    "\n",
    "submit = submit.sort_values(['proba'],ascending=False)\n",
    "submit = submit.reset_index(drop=True)\n",
    "submit['category_id'] = 1\n",
    "submit.loc[int(len(submit)*0.5):,'category_id'] = 0\n",
    "\n",
    "submit[['user_id', 'proba']].to_csv('./submit/sub_proba_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)\n",
    "submit[['user_id', 'category_id']].to_csv('./submit/sub_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "   user_id       tmp     rank  category_id\n",
      "0  1400001  0.470601  50171.0            1\n",
      "1  1400002  0.307447  29450.0            0\n",
      "2  1400003  0.363748  36186.0            0\n",
      "3  1400004  0.489368  52677.0            1\n",
      "4  1400005  0.071545   5443.0            0\n"
     ]
    }
   ],
   "source": [
    "# submit = test[['pid']]\n",
    "# submit['proba'] = predictions\n",
    "# submit.columns = ['user_id', 'proba']\n",
    "\n",
    "# submit['rank'] = submit['proba'].rank()\n",
    "# submit['category_id'] = 1\n",
    "# submit.loc[submit['rank'] <= int(submit.shape[0] * 0.5), 'category_id'] = 0\n",
    "\n",
    "#submit[['user_id', 'proba']].to_csv('./sub/sub_proba_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)\n",
    "#submit[['user_id', 'category_id']].to_csv('./sub/sub_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
